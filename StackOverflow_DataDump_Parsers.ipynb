{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8403c10-f943-42b3-9636-ebed66e62a3d",
   "metadata": {},
   "source": [
    "<h1>Code Snippets for Parsing the Gigantic Stack Overflow Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c7177-8c13-4f9f-9c5b-a4cab14e769a",
   "metadata": {},
   "source": [
    "<p><b>Step1:</b> <u>Download the data dump from the official Stack Exchange <a href='https://archive.org/download/stackexchange'>archive</a> if you want to have a data dump version published around April 2024. If you want to download the latest version you need to follow the instruction <a href='https://stackoverflow.com/help/data-dumps'>here</a>.</u> It contains 8 XML files namely Badges, Comments, PostHistory, PostLinks, Posts, Tags, Users and Votes.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735568f9-820e-4974-98e3-bd6fd19e14d8",
   "metadata": {},
   "source": [
    "<p><b>Step 2: </b><u>Split the large sized XML files to smaller chunks</u>. Apart from PostLinks, and Tags, all other files are too large to process easily in one go. So the next step is to divide them into small chunks. For this purpose, <a href='https://git-scm.com/download/win'>git bash</a> can be used which provides split command to split files line by line or by size. We used the split by line option. After extracting each of the above mentioned compressed files, open git bash terminal in the respective root directory and type the following commands respectively. </p><br>\n",
    "<ul>time split -l 50000 -d -a 5 Badges.xml Badges/Split/Badges</ul>\n",
    "<ul>time split -l 50000 -d -a 5 Users.xml  Users/Split/Users</ul>\n",
    "<ul>time split -l 50000 -d -a 5 Comments.xml  Comments/Split/Comments</ul>\n",
    "<ul>time split -l 50000 -d -a 5 Votes.xml  Votes/Split/Votes</ul>\n",
    "<ul>time split -l 50000 -d -a 5 Posts.xml  Posts/Split/Posts</ul>\n",
    "<ul>time split -l 50000 -d -a 5 PostHistory.xml  PostHistory/Split/PostHistory</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63f087-644f-4a9f-9734-026e079fe46e",
   "metadata": {},
   "source": [
    "<p><b>Step 3: </b><u>Converting splitted files to proper XML files.</u> As the split operation was performed line by line, the first chunk contains XML header whereas the last chunk contains the closing tag of the respective file. The rest of the chunks lack proper XML header/footer. This Python script is used to add these missing XML header/footer tags to convert the chunks to proper XML files. After running the above command (e.g. <code>split -l 50000 -d -a 10 Posts.xml Posts/Split/Posts</code>, the <code>Posts/Split</code> folder will contain all the generated chunks so we need to provide thhe full path to this folder as <code>input_path</code>. We used the following code snippet to convert these chunks into small sized XML files. Replace the XML header/footer strings in the code snippet (e.g <code> posts </code> and <code> /posts </code>) based on the file that you are dealing with e.g.  users, votes, posthistory, badges or comments and change the input and output paths accordingly. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b11c3-a695-487c-8eed-b266b68b32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "input_path = r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\Posts\\Split'\n",
    "output_path = r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\Posts\\Split\\\\'\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# ---- Get files in sorted order ----\n",
    "files = sorted(glob.glob(os.path.join(input_path, '*')))\n",
    "\n",
    "total_files = len(files)\n",
    "\n",
    "if total_files == 0:\n",
    "    print(\"No input files found.\")\n",
    "    exit()\n",
    "\n",
    "for idx, filename in enumerate(files):\n",
    "\n",
    "    print(f\"Reading file {filename}\")\n",
    "\n",
    "    # Determine first and last file\n",
    "    is_first = (idx == 0)\n",
    "    is_last = (idx == total_files - 1)\n",
    "\n",
    "    head, tail = os.path.split(filename)\n",
    "    outputfile = output_path + tail + '.xml'\n",
    "\n",
    "    print(f\"Writing file to {outputfile}\")\n",
    "\n",
    "    with open(filename, 'r', encoding='utf8') as f, \\\n",
    "         open(outputfile, \"w\", encoding='utf8') as output:\n",
    "\n",
    "        # ---- Add opening tag (skip first file) ----\n",
    "        if not is_first:\n",
    "            output.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\\n<posts>\")\n",
    "\n",
    "        # ---- Stream file content (memory safe) ----\n",
    "        for line in f:\n",
    "            output.write(line)\n",
    "\n",
    "        # ---- Add closing tag (skip last file) ----\n",
    "        if not is_last:\n",
    "            output.write(\"</posts>\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614efa8-051f-475d-8769-28c8f30e5868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42ae8db3-9422-44f3-8689-ddb65af753d3",
   "metadata": {},
   "source": [
    "<p><b>Step 4: </b><u>Saving XML based data in database for further querying.</u> We used MS SQL Server Developer Edition to store the data as the Express Edition restricts the database size to 10GB at maximum. However, in our case that dataset was much larger than that. It is worth mentioning here that the Developer Edition cannot be used in production environment. We further used SQL Server Management Studio (SSMS) to easily create and manipulate databases using a GUI. Both <a href='https://www.microsoft.com/en-us/sql-server/sql-server-downloads?msockid=236f7f3cd0c269f836b36be8d4c26fdb'>SQL Server </a> and <a href='https://learn.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver16'> SSMS </a> are available for use freely by Microsoft. The default location used by SQL Server to store database files on a Windows based system is <code>C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\DATA\\</code>. Using <code>New Database</code> wizard in SSMS, create new database and change the default location for storing the dataabase files to some other directory as your <code>C:</code> drive may have limited space. Instead of storing all data in a single database, we created separate databases corresponding to each of the above mentioned XML files (i.e. <code> StackOverflowBadgesDb_Jan2026, StackOverflowCommentsDb_Jan2026, StackOverflowPostHistoryDb_Jan2026, StackOverflowPostLinksDb_Jan2026, StackOverflowPostsDb_Jan2026, StackOverflowTagsDb_Jan2026, StackOverflowUsersDb_Jan2026, StackOverflowVotesDb_Jan2026 </code> respectively.)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc395bfc-8bf4-49e2-8351-9279649dc27a",
   "metadata": {},
   "source": [
    "<p><b>Step 5: </b><u>Creating Tables for Storing Data</u>. After creating the respective databases, the next step is to create tables in which the actual data will be stored from the XML files. We used the following SQL queries to create corresponding tables. </p>\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83e4f087-6348-4a29-8ff1-a70b8b81c95d",
   "metadata": {},
   "source": [
    "\n",
    "use  StackOverflowBadgesDb_Jan2026\n",
    "create table StackOverflowBadges (AutoIdPK int Identity(1,1) NOT NULL,\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tUserId int, \n",
    "\t\t\t\tName nvarchar(50),\n",
    "\t\t\t\tDate datetime,\n",
    "\t\t\t\tClass tinyint, \n",
    "\t\t\t\tTagBased bit,\n",
    "                CONSTRAINT PK_AutoIdPK PRIMARY KEY CLUSTERED (AutoIdPK))\n",
    "\n",
    "use  StackOverflowCommentsDb_Jan2026\n",
    "create table StackOverflowComments (AutoIdPK int Identity(1,1) NOT NULL,\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tPostId int,\n",
    "\t\t\t\tScore int,\n",
    "\t\t\t\tText nvarchar(600),\n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tUserDisplayName nvarchar(40),\n",
    "\t\t\t\tUserId int,\n",
    "\t\t\t\tContentLicense varchar(30),\n",
    "\t\t\t\tCONSTRAINT PK_AutoIdPK PRIMARY KEY CLUSTERED (AutoIdPK))\n",
    "\n",
    "use  StackOverflowPostHistoryDb_Jan2026\n",
    "create table StackOverflowPostHistory (AutoIdPK int Identity(1,1) NOT NULL,\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tPostHistoryTypeId tinyint,\n",
    "\t\t\t\tPostId int,\n",
    "\t\t\t\tRevisionGUID uniqueidentifier,\n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tUserId int, \n",
    "\t\t\t\tUserDisplayName nvarchar(400),\n",
    "\t\t\t\tComment nvarchar(400),\n",
    "\t\t\t\tText nvarchar(max),\n",
    "\t\t\t\tContentLicense varchar(30),\n",
    "                CONSTRAINT PK_AutoIdPK PRIMARY KEY CLUSTERED (AutoIdPK))\n",
    "\n",
    "use  StackOverflowPostLinksDb_Jan2026\n",
    "create table StackOverflowPostLinks (AutoIdPK int Identity(1,1) NOT NULL,\n",
    "\t\t\t\tId bigint, \n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tPostId int, \n",
    "\t\t\t\tRelatedPostId int,\n",
    "\t\t\t\tLinkTypeId tinyint,\n",
    "                CONSTRAINT PK_AutoIdPK PRIMARY KEY CLUSTERED (AutoIdPK))\n",
    "\n",
    "use  StackOverflowPostsDb_Jan2026\n",
    "create table StackOverflowPosts (AutoIdPK int Identity(1,1) NOT NULL,\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tPostTypeId tinyint,\n",
    "\t\t\t\tAcceptedAnswerId int,\n",
    "\t\t\t\tParentId int,\n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tDeletionDate datetime, \n",
    "\t\t\t\tScore int,\n",
    "\t\t\t\tViewCount int,\n",
    "\t\t\t\tBody nvarchar(max),\n",
    "\t\t\t\tOwnerUserId int,\n",
    "\t\t\t\tOwnerDisplayName nvarchar(40),\n",
    "\t\t\t\tLastEditorUserId int,\n",
    "\t\t\t\tLastEditorDisplayName nvarchar(40), \n",
    "\t\t\t\tLastEditDate datetime,\n",
    "\t\t\t\tLastActivityDate datetime,\n",
    "\t\t\t\tTitle nvarchar(250),\n",
    "\t\t\t\tTags nvarchar(4000),\n",
    "\t\t\t\tAnswerCount int,\n",
    "\t\t\t\tCommentCount int, \n",
    "\t\t\t\tFavoriteCount int, \n",
    "\t\t\t\tClosedDate datetime,\n",
    "\t\t\t\tCommunityOwnedDate datetime,\n",
    "\t\t\t\tContentLicense varchar(30), \n",
    "                CONSTRAINT PK_AutoIdPK PRIMARY KEY CLUSTERED (AutoIdPK)\n",
    "\t\t\t\t)\n",
    "\n",
    "use  StackOverflowTagsDb_Jan2026\n",
    "create table StackOverflowTags (AutoIdPK int Identity(1,1) NOT NULL,\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tTagName nvarchar(35),\n",
    "\t\t\t\tTagCount int, \n",
    "\t\t\t\tExcerptPostId int,\n",
    "\t\t\t\tWikiPostId int,\n",
    "                CONSTRAINT PK_AutoIdPK PRIMARY KEY CLUSTERED (AutoIdPK))\n",
    "\n",
    "use  StackOverflowUsersDb_Jan2026\n",
    "create table StackOverflowUsers (AutoIdPK int Identity(1,1) NOT NULL,\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tReputation int, \n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tDisplayName nvarchar(40),\n",
    "\t\t\t\tLastAccessDate datetime,\n",
    "\t\t\t\tWebsiteUrl nvarchar(200), \n",
    "\t\t\t\tLocation nvarchar(100), \n",
    "\t\t\t\tAboutMe nvarchar(max),\n",
    "\t\t\t\tViews int, \n",
    "\t\t\t\tUpvotes int, \n",
    "\t\t\t\tDownVotes int, \n",
    "\t\t\t\tProfileImageUrl nvarchar(200), \n",
    "\t\t\t\tAccountId int,\n",
    "                CONSTRAINT PK_AutoIdPK PRIMARY KEY CLUSTERED (AutoIdPK))\n",
    "\n",
    "use  StackOverflowVotesDb_Jan2026\n",
    "create table StackOverflowVotes (AutoIdPK int Identity(1,1) NOT NULL,\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tPostId int, \n",
    "\t\t\t\tVoteTypeId tinyint,\n",
    "\t\t\t\tUserId int,\n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tBountyAmount int,\n",
    "                CONSTRAINT PK_AutoIdPK PRIMARY KEY CLUSTERED (AutoIdPK))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe02e8-00b7-4a53-b558-5ba7f633666b",
   "metadata": {},
   "source": [
    "<p><b>Step 6: </b><u>Reading XML files and inserting records in respective database tables</u>. Now we need to read the small sized XML files one by one and store the data in the respective tables created above. For this purpose, the code snippet used for parsing each of the 8 XML file types namely Badges, Comments, PostHistory, PostLinks, Posts, Tags, Users, and Votes is given below. Before using these snippets, replace the server and database names and the path of directory containing splitted XML files. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a4558b-c710-46de-8505-19668ea03c9a",
   "metadata": {},
   "source": [
    "<h2>Badges Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac48e0f-39f4-4170-8a05-9f112a27e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "server = 'localhost'\n",
    "database = 'StackOverflowBadgesDb_Jan2026' \n",
    "start_time = time.perf_counter()\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "badges_xml_path =r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\Badges\\SplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(badges_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    \n",
    "    badges_df = pd.read_xml(filename)\n",
    "    badges_df = badges_df.fillna(0)\n",
    "\n",
    "    # print (badges_df.head())\n",
    "    # print('*************')\n",
    " \n",
    "    query = \"insert into StackOverflowBadges (Id, UserId, Name, Date, Class,TagBased) values (?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, badges_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")\n",
    "    batch_time = time.perf_counter()\n",
    "    print(f\"\\nTime taken till now {batch_time - start_time:.2f} seconds\")\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a98bc0-a72b-4933-b5a2-862f62240504",
   "metadata": {},
   "source": [
    "<h2>Comments Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcd9e4-0649-43b2-b8de-19a7fc6e79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "server = 'localhost'\n",
    "database = 'StackOverflowCommentsDb_Jan2026' \n",
    "start_time = time.perf_counter()\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "comments_xml_path =r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\Comments\\SplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(comments_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    comments_df = pd.read_xml(filename)\n",
    "    comments_df = comments_df.fillna(0)\n",
    "\n",
    "    # print (comments_df.head())\n",
    "    # print('*************')\n",
    "    columnnames= list(comments_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "    # query=\"insert into StackOverflowComments (\"+columns+\") values (?,?,?,?,?,?,?,?)\"\n",
    "    placeholders = \",\".join([\"?\"] * len(columnnames))\n",
    "\n",
    "    query = f\"insert into StackOverflowComments ({columns}) VALUES ({placeholders})\"\n",
    "    \n",
    "    cursor.executemany(query, comments_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")\n",
    "    batch_time = time.perf_counter()\n",
    "    print(f\"\\nTime taken till now {batch_time - start_time:.2f} seconds\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318dd54-abd0-4591-8eb2-9c6820a2b214",
   "metadata": {},
   "source": [
    "<h2>PostHistory Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44966145-7ad9-431b-b914-1b481fd96a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "server = 'localhost'\n",
    "database = 'StackOverflowPostHistoryDb_Jan2026' \n",
    "start_time = time.perf_counter()\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "post_history_xml_path ='E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\PostHistory\\SplitXML\\\\'\n",
    "\n",
    "for filename in glob.glob(os.path.join(post_history_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    PostHistory_df = pd.read_xml(filename)\n",
    "    \n",
    "    PostHistory_df =  PostHistory_df.fillna(value={'Id':0,'PostHistoryTypeId':0,'PostId':0,'RevisionGUID':0,'CreationDate':0,'UserId':0,'Text':'','ContentLicense':'','UserDisplayName':'', 'Comment':''})\n",
    "    # print (PostHistory_df.head())\n",
    "    # print('*************')\n",
    "    columnnames= list(PostHistory_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "    query=\"insert into StackOverflowPostHistory (\"+columns+\") values (?,?,?,?,?,?,?,?,?,?)\"\n",
    "    \n",
    "    # query = \"insert into StackOverflowPostHistory(Id,PostHistoryTypeId,PostId,RevisionGUID,CreationDate,UserId,Text,ContentLicense,Comment,UserDisplayName) values (?,?,?,?,?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, PostHistory_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")\n",
    "    batch_time = time.perf_counter()\n",
    "    print(f\"\\nTime taken till now {batch_time - start_time:.2f} seconds\")\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449b1a3-f743-471c-962d-0424ce8e1513",
   "metadata": {},
   "source": [
    "<h2>PostLinks Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b43d60-57f5-4825-8678-2b545f10556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "server = 'localhost'\n",
    "database = 'StackOverflowPostLinksDb_Jan2026' \n",
    "start_time = time.perf_counter()\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "PostLinks_xml_path =r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\PostLinks.xml'\n",
    "PostLinks_df = pd.read_xml(PostLinks_xml_path)\n",
    "PostLinks_df = PostLinks_df.fillna(0)\n",
    "\n",
    "print (PostLinks_df.head())\n",
    "print('*************')\n",
    " \n",
    "query = \"insert into StackOverflowPostLinks (Id,CreationDate,PostId,RelatedPostId,LinkTypeId) values (?,?,?,?,?)\"\n",
    "\n",
    "cursor.executemany(query, PostLinks_df.values.tolist())\n",
    "cursor.commit()\n",
    "print(\"Added in database\")\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce38fa4-b620-421d-853e-0cab097957d3",
   "metadata": {},
   "source": [
    "<h2>Posts Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138f4f4-bfd0-47c2-841f-427f4422bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "server = 'localhost'\n",
    "database = 'StackOverflowPostsDb_Jan2026' \n",
    "start_time = time.perf_counter()\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "posts_xml_path =r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\Posts\\SplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(posts_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    posts_df = pd.read_xml(filename)\n",
    "    # print(posts_df.head())\n",
    "    posts_df =  posts_df.fillna(value={'Id':0,'PostTypeId':0,'AcceptedAnswerId':0,'CreationDate':'','Score':0,'ViewCount':0,'OwnerUserId':0, 'LastEditorUserId':0, 'LastEditDate':0, 'LastActivityDate':0,'AnswerCount':0,'CommentCount':0, 'ParentId':0, 'CommunityOwnedDate':0 ,  'ClosedDate':0,\n",
    "                            'FavoriteCount':0})\n",
    "    posts_df['Id']=posts_df['Id'].astype(int)\n",
    "    posts_df['PostTypeId']=posts_df['PostTypeId'].astype(int)\n",
    "    if 'AcceptedAnswerId' in posts_df.columns:\n",
    "        posts_df['AcceptedAnswerId']=posts_df['AcceptedAnswerId'].astype(int)\n",
    "    posts_df['CreationDate']= pd.to_datetime(posts_df['CreationDate'], format=\"mixed\")\n",
    "    posts_df['Score']=posts_df['Score'].astype(int)\n",
    "    if 'ViewCount' in posts_df.columns:\n",
    "        posts_df['ViewCount']=posts_df['ViewCount'].astype(int)\n",
    "    posts_df['OwnerUserId']=posts_df['OwnerUserId'].astype(int)\n",
    "    posts_df['LastEditorUserId']=posts_df['LastEditorUserId'].astype(int)\n",
    "    posts_df['LastEditDate']= pd.to_datetime(posts_df['LastEditDate'], format=\"mixed\")\n",
    "    posts_df['LastActivityDate']= pd.to_datetime(posts_df['LastActivityDate'], format=\"mixed\")\n",
    "    if 'AnswerCount' in posts_df.columns:\n",
    "        posts_df['AnswerCount']=posts_df['AnswerCount'].astype(int)\n",
    "    if 'CommentCount' in posts_df.columns:\n",
    "        posts_df['CommentCount']=posts_df['CommentCount'].astype(int)\n",
    "    if 'ParentId' in posts_df.columns:\n",
    "        posts_df['ParentId']=posts_df['ParentId'].astype(int)\n",
    "    if 'CommunityOwnedDate' in posts_df.columns:\n",
    "        posts_df['CommunityOwnedDate']= pd.to_datetime(posts_df['CommunityOwnedDate'], format=\"mixed\")\n",
    "    if 'ClosedDate' in posts_df.columns:\n",
    "        posts_df['ClosedDate']= pd.to_datetime(posts_df['ClosedDate'], format=\"mixed\")\n",
    "\n",
    "    if 'FavoriteCount' in posts_df.columns:\n",
    "        posts_df['FavoriteCount']=posts_df['FavoriteCount'].astype(int)\n",
    "    columnnames= list(posts_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "    placeholders = \",\".join([\"?\"] * len(columnnames))\n",
    "\n",
    "    query = f\"insert into StackOverflowPosts ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    posts_df = posts_df.fillna('')\n",
    "\n",
    "    cursor.executemany(query, posts_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")\n",
    "    batch_time = time.perf_counter()\n",
    "    print(f\"\\nTime taken till now {batch_time - start_time:.2f} seconds\")\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e0669-3296-40ec-b20f-2d0b9aff141e",
   "metadata": {},
   "source": [
    "<h2>Tags Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c408f-fe9a-413b-aafe-f6515d2df5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "server = 'localhost'\n",
    "database = 'StackOverflowTagsDb_Jan2026' \n",
    "start_time = time.perf_counter()\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "row_list = []\n",
    "errorcount = 0\n",
    "tags_xml_path =r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\Tags.xml'\n",
    "tags_df = pd.read_xml(tags_xml_path)\n",
    "tags_df = tags_df.fillna('')\n",
    "\n",
    "print (tags_df.head())\n",
    "print('*************')\n",
    "\n",
    "query = \"insert into StackOverflowTags (Id, TagName, TagCount ,ExcerptPostId ,WikiPostId) values (?,?,?,?,?)\"\n",
    "\n",
    "cursor.executemany(query, tags_df.values.tolist())\n",
    "cursor.commit()\n",
    "print(\"Added in database\")\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fe9ae-4db3-4eb8-9c8a-8124675c0c62",
   "metadata": {},
   "source": [
    "<h2>Users Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965dd0b-2885-41b1-b751-d39c19169e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "server = 'localhost'\n",
    "database = 'StackOverflowUsersDb_Jan2026' \n",
    "start_time = time.perf_counter()\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "users_xml_path =r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\Users\\SplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(users_xml_path, '*')):\n",
    "    print(filename)\n",
    "    users_df = pd.read_xml(filename)\n",
    "    users_df = users_df.fillna(0)\n",
    "    \n",
    "    # print (users_df.head())\n",
    "    # print(users_df.columns)\n",
    "    # print('*************')\n",
    "    columnnames= list(users_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "#     print(columnnames)\n",
    "    query=\"insert into StackOverflowUsers (\"+columns+\") values (?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "#     query = \"insert into StackOverflowUsers ( Id, Reputation, CreationDate, DisplayName, LastAccessDate, Views, UpVotes, DownVotes, AccountId, WebsiteUrl, AboutMe,ProfileImageUrl, Location) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, users_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")\n",
    "    batch_time = time.perf_counter()\n",
    "    print(f\"\\nTime taken till now {batch_time - start_time:.2f} seconds\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73a0ed-e822-4166-be6b-c84588c549b6",
   "metadata": {},
   "source": [
    "<h2>Votes Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bfcb7-b395-4a82-81d4-99683a674a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "import time \n",
    "server = 'localhost'\n",
    "database = 'StackOverflowVotesDb_Jan2026' \n",
    "start_time = time.perf_counter()\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "votes_xml_path =r'E:\\Dataset_2026\\StackOverflow\\Jan2026\\stackoverflow.com\\Votes\\SplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(votes_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    votes_df = pd.read_xml(filename)\n",
    "    votes_df = votes_df.fillna(0)\n",
    "\n",
    "    # print (votes_df.head())\n",
    "    # print('*************')\n",
    "    columnnames= list(votes_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "    if 'UserId' in votes_df.columns and 'BountyAmount' in votes_df.columns :\n",
    "        votes_df['UserId']=votes_df['UserId'].astype(int)\n",
    "        votes_df['BountyAmount']=votes_df['BountyAmount'].astype(int)\n",
    "        \n",
    "        query=\"insert into StackOverflowVotes (\"+columns+\") values (?,?,?,?,?,?)\"\n",
    "    else:\n",
    "        query = \"insert into StackOverflowVotes (Id, PostId,VoteTypeId,CreationDate) values (?,?,?,?)\"\n",
    "        \n",
    "    cursor.executemany(query, votes_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")\n",
    "    batch_time = time.perf_counter()\n",
    "    print(f\"\\nTime taken till now {batch_time - start_time:.2f} seconds\")\n",
    "end_time = time.perf_counter()\n",
    "print(f\"\\nCompleted in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4456d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef4f6a-408e-4889-8c8a-e094d7a2eb34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
